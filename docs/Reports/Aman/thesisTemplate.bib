@inproceedings{ahir,
author = {Sahasrabuddhe, Sameer and Raja, Hakim and Arya, Kavi and Desai, Madhav},
year = {2007},
month = {01},
pages = {245-250},
title = {AHIR: A Hardware Intermediate Representation for Hardware Generation from High-level Programs},
doi = {10.1109/VLSID.2007.28}
}
@online{convImg,
  author = {Ajitesh Kumar},
  title = {Real-World Applications of Convolutional Neural Networks},
  year = 2021,
  url = {https://vitalflux.com/real-world-applications-of-convolutional-neural-networks/},
  urldate = {2021-11-06}
}

@InProceedings{UNET,
author="Ronneberger, Olaf
and Fischer, Philipp
and Brox, Thomas",
editor="Navab, Nassir
and Hornegger, Joachim
and Wells, William M.
and Frangi, Alejandro F.",
title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="234--241",
abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
isbn="978-3-319-24574-4"
}

@article{CNNs,
  author       = {Keiron O'Shea and
                  Ryan Nash},
  title        = {An Introduction to Convolutional Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1511.08458},
  year         = {2015},
  url          = {http://arxiv.org/abs/1511.08458},
  eprinttype    = {arXiv},
  eprint       = {1511.08458},
  timestamp    = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/OSheaN15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{SVMs,

  author={Hearst, M.A. and Dumais, S.T. and Osuna, E. and Platt, J. and Scholkopf, B.},

  journal={IEEE Intelligent Systems and their Applications}, 

  title={Support vector machines}, 

  year={1998},

  volume={13},

  number={4},

  pages={18-28},

  doi={10.1109/5254.708428}}

  @ARTICLE{LeNet,

  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},

  journal={Proceedings of the IEEE},

  title={Gradient-based learning applied to document recognition},

  year={1998},

  volume={86},

  number={11},

  pages={2278-2324},

  doi={10.1109/5.726791}}

  @inproceedings{AlexNet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1097–1105},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@INPROCEEDINGS{GoogleNet,

  author={Szegedy, Christian and Wei Liu and Yangqing Jia and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},

  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Going deeper with convolutions}, 

  year={2015},

  volume={},

  number={},

  pages={1-9},

  doi={10.1109/CVPR.2015.7298594}}


 @INPROCEEDINGS{ResNet,

  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},

  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Deep Residual Learning for Image Recognition}, 

  year={2016},

  volume={},

  number={},

  pages={770-778},

  doi={10.1109/CVPR.2016.90}}

@INPROCEEDINGS{YOLO,
author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={You Only Look Once: Unified, Real-Time Object Detection},
year={2016},
volume={},
number={},
pages={779-788},
doi={10.1109/CVPR.2016.91}}
@INPROCEEDINGS{CNN1,
author={Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Improved Texture Networks: Maximizing Quality and Diversity in Feed-Forward Stylization and Texture Synthesis},
year={2017},
volume={},
number={},
pages={4105-4113},
doi={10.1109/CVPR.2017.437}}
@INPROCEEDINGS{CNN2,
author={Huang, Xun and Belongie, Serge},
booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},
title={Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization},
year={2017},
volume={},
number={},
pages={1510-1519},
doi={10.1109/ICCV.2017.167}}
@INPROCEEDINGS{CNN3,
author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Image-to-Image Translation with Conditional Adversarial Networks},
year={2017},
volume={},
number={},
pages={5967-5976},
doi={10.1109/CVPR.2017.632}}
@INPROCEEDINGS{CNN4,
author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},
title={Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks},
year={2017},
volume={},
number={},
pages={2242-2251},
doi={10.1109/ICCV.2017.244}}
@INPROCEEDINGS{CNN5,
author={Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
title={StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation},
year={2018},
volume={},
number={},
pages={8789-8797},
doi={10.1109/CVPR.2018.00916}}
@INPROCEEDINGS{CNN6,
author={Shi, Wenzhe and Caballero, Jose and Huszár, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network},
year={2016},
volume={},
number={},
pages={1874-1883},
doi={10.1109/CVPR.2016.207}}
@INPROCEEDINGS{CNN7,
author={Kim, Jiwon and Lee, Jung Kwon and Lee, Kyoung Mu},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Accurate Image Super-Resolution Using Very Deep Convolutional Networks},
year={2016},
volume={},
number={},
pages={1646-1654},
doi={10.1109/CVPR.2016.182}}


@inproceedings{TPU,
title	= {In-Datacenter Performance Analysis of a Tensor Processing Unit},
author	= {Norman P. Jouppi and Cliff Young and Nishant Patil and David Patterson and Gaurav Agrawal and Raminder Bajwa and Sarah Bates and Suresh Bhatia and Nan Boden and Al Borchers and Rick Boyle and Pierre-luc Cantin and Clifford Chao and Chris Clark and Jeremy Coriell and Mike Daley and Matt Dau and Jeffrey Dean and Ben Gelb and Tara Vazir Ghaemmaghami and Rajendra Gottipati and William Gulland and Robert Hagmann and C. Richard Ho and Doug Hogberg and John Hu and Robert Hundt and Dan Hurt and Julian Ibarz and Aaron Jaffey and Alek Jaworski and Alexander Kaplan and Harshit Khaitan and Andy Koch and Naveen Kumar and Steve Lacy and James Laudon and James Law and Diemthu Le and Chris Leary and Zhuyuan Liu and Kyle Lucke and Alan Lundin and Gordon MacKean and Adriana Maggiore and Maire Mahony and Kieran Miller and Rahul Nagarajan and Ravi Narayanaswami and Ray Ni and Kathy Nix and Thomas Norrie and Mark Omernick and Narayana Penukonda and Andy Phelps and Jonathan Ross},
year	= {2017},
URL	= {https://arxiv.org/pdf/1704.04760.pdf}
}

@inproceedings{FPGA1,
author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason},
title = {Optimizing FPGA-Based Accelerator Design for Deep Convolutional Neural Networks},
year = {2015},
isbn = {9781450333153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684746.2689060},
doi = {10.1145/2684746.2689060},
abstract = {Convolutional neural network (CNN) has been widely employed for image recognition because it can achieve high accuracy by emulating behavior of optic nerves in living creatures. Recently, rapid growth of modern applications based on deep learning algorithms has further improved research and implementations. Especially, various accelerators for deep CNN have been proposed based on FPGA platform because it has advantages of high performance, reconfigurability, and fast development round, etc. Although current FPGA accelerators have demonstrated better performance over generic processors, the accelerator design space has not been well exploited. One critical problem is that the computation throughput may not well match the memory bandwidth provided an FPGA platform. Consequently, existing approaches cannot achieve best performance due to under-utilization of either logic resource or memory bandwidth. At the same time, the increasing complexity and scalability of deep learning applications aggravate this problem. In order to overcome this problem, we propose an analytical design scheme using the roofline model. For any solution of a CNN design, we quantitatively analyze its computing throughput and required memory bandwidth using various optimization techniques, such as loop tiling and transformation. Then, with the help of rooine model, we can identify the solution with best performance and lowest FPGA resource requirement. As a case study, we implement a CNN accelerator on a VC707 FPGA board and compare it to previous approaches. Our implementation achieves a peak performance of 61.62 GFLOPS under 100MHz working frequency, which outperform previous approaches significantly.},
booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {161–170},
numpages = {10},
keywords = {roofline model, fpga, acceleration, convolutional neural network},
location = {Monterey, California, USA},
series = {FPGA '15}
}

@inproceedings{FPGA2,
author = {Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi and Song, Sen and Wang, Yu and Yang, Huazhong},
title = {Going Deeper with Embedded FPGA Platform for Convolutional Neural Network},
year = {2016},
isbn = {9781450338561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2847263.2847265},
doi = {10.1145/2847263.2847265},
abstract = {In recent years, convolutional neural network (CNN) based methods have achieved great success in a large number of applications and have been among the most powerful and widely used techniques in computer vision. However, CNN-based methods are com-putational-intensive and resource-consuming, and thus are hard to be integrated into embedded systems such as smart phones, smart glasses, and robots. FPGA is one of the most promising platforms for accelerating CNN, but the limited bandwidth and on-chip memory size limit the performance of FPGA accelerator for CNN.In this paper, we go deeper with the embedded FPGA platform on accelerating CNNs and propose a CNN accelerator design on embedded FPGA for Image-Net large-scale image classification. We first present an in-depth analysis of state-of-the-art CNN models and show that Convolutional layers are computational-centric and Fully-Connected layers are memory-centric.Then the dynamic-precision data quantization method and a convolver design that is efficient for all layer types in CNN are proposed to improve the bandwidth and resource utilization. Results show that only 0.4\% accuracy loss is introduced by our data quantization flow for the very deep VGG16 model when 8/4-bit quantization is used. A data arrangement method is proposed to further ensure a high utilization of the external memory bandwidth. Finally, a state-of-the-art CNN, VGG16-SVD, is implemented on an embedded FPGA platform as a case study. VGG16-SVD is the largest and most accurate network that has been implemented on FPGA end-to-end so far. The system on Xilinx Zynq ZC706 board achieves a frame rate at 4.45 fps with the top-5 accuracy of 86.66\% using 16-bit quantization. The average performance of convolutional layers and the full CNN is 187.8 GOP/s and 137.0 GOP/s under 150MHz working frequency, which outperform previous approaches significantly.},
booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {26–35},
numpages = {10},
keywords = {convolutional neural network (cnn), embedded fpga, bandwidth utilization, dynamic-precision data quantization},
location = {Monterey, California, USA},
series = {FPGA '16}
}



@Article{FPGA3,
AUTHOR = {Cho, Mannhee and Kim, Youngmin},
TITLE = {FPGA-Based Convolutional Neural Network Accelerator with Resource-Optimized Approximate Multiply-Accumulate Unit},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {2859},
URL = {https://www.mdpi.com/2079-9292/10/22/2859},
ISSN = {2079-9292},
ABSTRACT = {Convolutional neural networks (CNNs) are widely used in modern applications for their versatility and high classification accuracy. Field-programmable gate arrays (FPGAs) are considered to be suitable platforms for CNNs based on their high performance, rapid development, and reconfigurability. Although many studies have proposed methods for implementing high-performance CNN accelerators on FPGAs using optimized data types and algorithm transformations, accelerators can be optimized further by investigating more efficient uses of FPGA resources. In this paper, we propose an FPGA-based CNN accelerator using multiple approximate accumulation units based on a fixed-point data type. We implemented the LeNet-5 CNN architecture, which performs classification of handwritten digits using the MNIST handwritten digit dataset. The proposed accelerator was implemented, using a high-level synthesis tool on a Xilinx FPGA. The proposed accelerator applies an optimized fixed-point data type and loop parallelization to improve performance. Approximate operation units are implemented using FPGA logic resources instead of high-precision digital signal processing (DSP) blocks, which are inefficient for low-precision data. Our accelerator model achieves 66% less memory usage and approximately 50% reduced network latency, compared to a floating point design and its resource utilization is optimized to use 78% fewer DSP blocks, compared to general fixed-point designs.},
DOI = {10.3390/electronics10222859}
}

@ARTICLE{17,
author={Yin, Shouyi and Tang, Shibin and Lin, Xinhan and Ouyang, Peng and Tu, Fengbin and Liu, Leibo and Wei, Shaojun},
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
title={A High Throughput Acceleration for Hybrid Neural Networks With Efficient Resource Management on FPGA},
year={2019},
volume={38},
number={4},
pages={678-691},
doi={10.1109/TCAD.2018.2821561}}
@ARTICLE{29,
author={Ma, Yufei and Cao, Yu and Vrudhula, Sarma and Seo, Jae-Sun},
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
title={Automatic Compilation of Diverse CNNs Onto High-Performance FPGA Accelerators},
year={2020},
volume={39},
number={2},
pages={424-437},
doi={10.1109/TCAD.2018.2884972}}

@ARTICLE{28,

  author={Huang, Wenjin and Wu, Huangtao and Chen, Qingkun and Luo, Conghui and Zeng, Shihao and Li, Tianrui and Huang, Yihua},

  journal={IEEE Transactions on Neural Networks and Learning Systems}, 

  title={FPGA-Based High-Throughput CNN Hardware Accelerator With High Computing Resource Utilization Ratio}, 

  year={2022},

  volume={33},

  number={8},

  pages={4069-4083},

  doi={10.1109/TNNLS.2021.3055814}}

  @ARTICLE{shawana_survey,

  author={Shawahna, Ahmad and Sait, Sadiq M. and El-Maleh, Aiman},

  journal={IEEE Access},

  title={FPGA-Based Accelerators of Deep Learning Networks for Learning and Classification: A Review},

  year={2019},

  volume={7},

  number={},

  pages={7823-7859},

  doi={10.1109/ACCESS.2018.2890150}}


@article{citeUNET1,
author = {Liu, Shuanglong and Fan, Hongxiang and Niu, Xinyu and Ng, Ho-cheung and Chu, Yang and LUK, Wayne},
title = {Optimizing CNN-Based Segmentation with Deeply Customized Convolutional and Deconvolutional Architectures on FPGA},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1936-7406},
url = {https://doi.org/10.1145/3242900},
doi = {10.1145/3242900},
abstract = {Convolutional Neural Networks-- (CNNs) based algorithms have been successful in solving image recognition problems, showing very large accuracy improvement. In recent years, deconvolution layers are widely used as key components in the state-of-the-art CNNs for end-to-end training and models to support tasks such as image segmentation and super resolution. However, the deconvolution algorithms are computationally intensive, which limits their applicability to real-time applications. Particularly, there has been little research on the efficient implementations of deconvolution algorithms on FPGA platforms that have been widely used to accelerate CNN algorithms by practitioners and researchers due to their high performance and power efficiency. In this work, we propose and develop deconvolution architecture for efficient FPGA implementation. FPGA-based accelerators are proposed for both deconvolution and CNN algorithms. Besides, memory sharing between the computation modules is proposed for the FPGA-based CNN accelerator as well as for other optimization techniques. A non-linear optimization model based on the performance model is introduced to efficiently explore the design space to achieve optimal processing speed of the system and improve power efficiency. Furthermore, a hardware mapping framework is developed to automatically generate the low-latency hardware design for any given CNN model on the target device. Finally, we implement our designs on Xilinx Zynq ZC706 board and the deconvolution accelerator achieves a performance of 90.1 giga operations per second (GOPS) under 200MHz working frequency and a performance density of 0.10 GOPS/DSP using 32-bit quantization, which significantly outperforms previous designs on FPGAs. A real-time application of scene segmentation on Cityscapes Dataset is used to evaluate our CNN accelerator on Zynq ZC706 board, and the system achieves a performance of 107 GOPS and 0.12 GOPS/DSP using 16-bit quantization and supports up to 17 frames per second for 512 \texttimes{} 512 image inputs with a power consumption of only 9.6W.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = {dec},
articleno = {19},
numpages = {22},
keywords = {deconvolution, convolutional neural networks (CNNs), segmentation, hardware acceleration, FPGA}
}

 @INPROCEEDINGS{layerCons,

  author={Wei, Xuechao and Liang, Yun and Cong, Jason},

  booktitle={2019 56th ACM/IEEE Design Automation Conference (DAC)}, 

  title={Overcoming Data Transfer Bottlenecks in FPGA-based DNN Accelerators via Layer Conscious Memory Management}, 

  year={2019},

  volume={},

  number={},

  pages={1-6},

  doi={}}

 @ARTICLE{UNIOPU,

  author={Yu, Yunxuan and Zhao, Tiandong and Wang, Mingyu and Wang, Kun and He, Lei},

  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},

  title={Uni-OPU: An FPGA-Based Uniform Accelerator for Convolutional and Transposed Convolutional Networks},

  year={2020},

  volume={28},

  number={7},

  pages={1545-1556},

  doi={10.1109/TVLSI.2020.2995741}}


@INPROCEEDINGS{citeUNET2,

  author={Liu, Shuanglong and Luk, Wayne},

  booktitle={2019 29th International Conference on Field Programmable Logic and Applications (FPL)}, 

  title={Towards an Efficient Accelerator for DNN-Based Remote Sensing Image Segmentation on FPGAs}, 

  year={2019},

  volume={},

  number={},

  pages={187-193},

  doi={10.1109/FPL.2019.00037}}

